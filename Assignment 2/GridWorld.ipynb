{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was adapted from a demo given by Andreas Krause in the course Probabilistic AI at ETH in 2021.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import copy \n",
    "\n",
    "from rllib.environment.mdps import EasyGridWorld\n",
    "from rllib.value_function import TabularValueFunction\n",
    "from rllib.policy import TabularPolicy\n",
    "\n",
    "environment = EasyGridWorld()\n",
    "gamma = 0.9\n",
    "\n",
    "# Plotters\n",
    "def policy2str(policy):\n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 == policy:\n",
    "        policy_str += down \n",
    "    if 1 == policy:\n",
    "        policy_str += up \n",
    "    if 2 == policy:\n",
    "        policy_str += right\n",
    "    if 3 == policy:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function, vmin=-1, vmax=30)\n",
    "    rows, cols = value_function.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax.text(row, col, f\"{value_function[col, row]:.1f}\", ha=\"center\", va=\"center\", color=\"w\", fontsize=24)\n",
    "\n",
    "def plot_policy(policy, ax):\n",
    "    rows, cols = policy.shape\n",
    "    ax.imshow(np.zeros((rows, cols)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            ax.text(col, row, policy2str(policy[row, col]), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "\n",
    "def plot_value_and_policy(value_function, policy):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 8))\n",
    "\n",
    "    plot_value_function(value_function, axes[0])\n",
    "    plot_policy(policy, axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637e890",
   "metadata": {},
   "source": [
    "## Task 1 - Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy_and_value_function():\n",
    "    ### ---Task 1.1--- ###\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE END  ###\n",
    "    \n",
    "    return policy, value_function\n",
    "\n",
    "def linear_system_policy_evaluation(environment, policy, gamma, value_function):\n",
    "\n",
    "    P, r = build_mrp_matrices(environment=environment, policy=policy)\n",
    "\n",
    "    ### ---Task 1.2--- ###\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE END  ###    \n",
    "    \n",
    "    for state in range(environment.num_states):\n",
    "        value_function.set_value(state, vals[state].item())\n",
    "\n",
    "    return value_function\n",
    "\n",
    "def build_mrp_matrices(environment, policy):\n",
    "    mrp_kernel = np.zeros((environment.num_states, environment.num_states))\n",
    "    mrp_reward = np.zeros((environment.num_states))\n",
    "\n",
    "    for state in range(environment.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        policy_ = Categorical(logits=policy(state))\n",
    "\n",
    "        for a, p_action in enumerate(policy_.probs):\n",
    "            for transition in environment.transitions[(state.item(), a)]:\n",
    "                with torch.no_grad():\n",
    "                    p_ns = transition[\"probability\"]\n",
    "                    mrp_reward[state] += p_action * p_ns * transition[\"reward\"]\n",
    "                    mrp_kernel[state, transition[\"next_state\"]\n",
    "                               ] += p_action * p_ns\n",
    "\n",
    "    return mrp_kernel, mrp_reward\n",
    "\n",
    "### ---Task 1.3--- ###\n",
    "policy, value_function = init_policy_and_value_function()\n",
    "value_function = linear_system_policy_evaluation(environment, policy, gamma, value_function)\n",
    "\n",
    "plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                      policy.table.argmax(0).reshape(5, 5).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86ac15",
   "metadata": {},
   "source": [
    "## Task 2 - Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69796cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "def policy_iteration(environment, gamma):\n",
    "    \"\"\"Implement Policy Iteration algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float.\n",
    "        discount factor.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.3\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    policy, value_function = init_policy_and_value_function()\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        # Evaluate the policy.\n",
    "        value_function = linear_system_policy_evaluation(environment, policy, gamma, value_function)\n",
    "\n",
    "        policy_stable = True\n",
    "        # iterate through all states\n",
    "        for state in range(environment.num_states):\n",
    "            # initialize values\n",
    "            value_ = torch.zeros(environment.num_actions)\n",
    "            \n",
    "            # iterate through possible actions\n",
    "            for action in range(environment.num_actions):\n",
    "                value_estimate = 0\n",
    "                \n",
    "                # iterate through transitions\n",
    "                for transition in environment.transitions[(state, action)]:\n",
    "                    next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                    reward = torch.tensor(transition[\"reward\"]).double()\n",
    "                    transition_probability = transition[\"probability\"]\n",
    "\n",
    "                    \n",
    "                    ### ---Task 2.1--- ###\n",
    "                    ### YOUR CODE HERE ###\n",
    "                    \n",
    "                    \n",
    "                    ### YOUR CODE END\n",
    "\n",
    "                value_[action] = value_estimate\n",
    "\n",
    "            state = torch.tensor(state).long()\n",
    "            old_policy = policy(state)\n",
    "            old_action = torch.argmax(old_policy)\n",
    "\n",
    "            action = torch.argmax(value_)\n",
    "            policy.set_value(state, action)\n",
    "            \n",
    "            ### ---Task 2.2--- ###\n",
    "            ### YOUR CODE HERE ###\n",
    "            \n",
    "            \n",
    "            ### YOUR CODE END  ###\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return value_function, policy\n",
    "\n",
    "### ---Task 2.3--- ###\n",
    "value_function, policy = policy_iteration(environment, gamma)\n",
    "plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                      policy.table.argmax(0).reshape(5, 5).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960fed6",
   "metadata": {},
   "source": [
    "## Task 3 - Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, gamma, eps=1e-6, max_iter=1000):\n",
    "    \"\"\"Implement of Value Iteration algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float\n",
    "        discount factor.\n",
    "    eps: float \n",
    "        desired precision.\n",
    "    max_iter: int \n",
    "        Max number of iterations. \n",
    "    value_function: TabularValueFunction, optional. \n",
    "        Initial value function. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    value_function: TabularValueFunction\n",
    "        Optimal value function.\n",
    "    policy: Tabular Policy \n",
    "        Optimal policy. \n",
    "    num_iter: int\n",
    "        Number of iterations to reach `eps' accuracy. \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.4\n",
    "\n",
    "    \"\"\"\n",
    "    policy, value_function = init_policy_and_value_function()\n",
    "\n",
    "    for num_iter in range(max_iter):\n",
    "        error = 0\n",
    "\n",
    "        old_value_function = copy.deepcopy(value_function)\n",
    "        for state in range(environment.num_states):\n",
    "            ### ---TASK 3.1--- ###\n",
    "            ### YOUR CODE HERE ###\n",
    "\n",
    "            \n",
    "            ### YOUR CODE END ###\n",
    "            \n",
    "            for action in range(environment.num_actions):\n",
    "                value_estimate = 0\n",
    "\n",
    "                # In practice, we do not have access to environment.transitions, but only to samples of it!.\n",
    "                for transition in environment.transitions[(state, action)]:  \n",
    "                    next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                    reward = torch.tensor(transition[\"reward\"]).double()\n",
    "                    transition_probability = transition[\"probability\"]\n",
    "\n",
    "                    ### ---TASK 3.2--- ###\n",
    "                    ### YOUR CODE HERE ###\n",
    "\n",
    "                    \n",
    "                    ### YOUR CODE END ###\n",
    "\n",
    "                \n",
    "            state = torch.tensor(state).long()\n",
    "            \n",
    "            ### ---TASK 3.3--- ###\n",
    "            ### YOUR CODE HERE ###\n",
    "\n",
    "            \n",
    "            ### YOUR CODE END ### \n",
    "            \n",
    "            policy.set_value(state, action)\n",
    "\n",
    "        if error < eps:\n",
    "            break\n",
    "\n",
    "    return value_function, policy, num_iter \n",
    "\n",
    "\n",
    "\n",
    "### ---Task 3.4--- ###\n",
    "value_function, policy, num_iter = value_iteration(environment, gamma)\n",
    "plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                      policy.table.argmax(0).reshape(5, 5).detach().numpy())\n",
    "print(f\"Iterations until convergence: {num_iter}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
