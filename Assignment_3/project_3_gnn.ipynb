{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>\n",
    "    Implementing, and using GNN architectures. <br/>\n",
    "  \n",
    "    \n",
    "    Project 3\n",
    "</center></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Question 1\n",
    "<img src=\"https://i.stack.imgur.com/HIV6k.png\" width=\"300\">\n",
    "\n",
    "Given the graph above with and without coloring, describe which machine learning algorithms preserve the topological information irrespective of the geometric visualization. Here and in the following different colored edges represent different edge relations. Please consider the graph that is visualized and not the picture of the graph.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can consider the graph witout colour. This is a fully connected graph, where each node is connected by an edge. In other words, every vertex in a fully connected graph is directly connected to every other vertex. Since there is no specific ordering, there are no permutations to consider. Therefore we can use a variety of machine learning approaches that work well for fully connected graphs where permutations are not an issue such as Graph Neural Networks (GNNs) and Deep Sets.  <br>\n",
    "\n",
    "Considering only the red or only the blue edges, not all nodes in the network are connected. Here there are other Networks that can be used, as permutational invariance is essential. Specifically Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs) are capable of working with non-fully connected graphs as they contain some permutation invariance. Graph Attention Networks combine elements of Transformers into their architecture to better process topological information with attention. A readout mechanism is used to generate a fixed-length representation of the entire graph (embedding) and in combination with the attention, permutation invariance can be achieved. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "Given the graph visualized in Theory Question 1. Visualize the graph in a datastructure of your choice, if your result does not match the geometry in the figure construct an isomorphism between the two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m red_graph \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[1;32m      2\u001b[0m                     [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m                     [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m                     [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m                     [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]])\n\u001b[1;32m      7\u001b[0m blue_graph \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      8\u001b[0m                     [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[1;32m      9\u001b[0m                     [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[1;32m     10\u001b[0m                     [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m     11\u001b[0m                     [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]])\n\u001b[1;32m     13\u001b[0m no_colour_graph \u001b[39m=\u001b[39m blue_graph \u001b[39m+\u001b[39m red_graph\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "red_graph = np.array([[0, 1, 0, 0, 1],\n",
    "                    [1, 0, 1, 0, 0],\n",
    "                    [0, 1, 0, 1, 0],\n",
    "                    [0, 0, 1, 0, 1],\n",
    "                    [1, 0, 0, 1, 0]])\n",
    "\n",
    "blue_graph = np.array([[0, 0, 1, 1, 0],\n",
    "                    [0, 0, 0, 1, 1],\n",
    "                    [1, 0, 0, 0, 1],\n",
    "                    [1, 1, 0, 0, 0],\n",
    "                    [0, 1, 1, 0, 0]])\n",
    "\n",
    "no_colour_graph = blue_graph + red_graph\n",
    "\n",
    "coloured_graph = red_graph - blue_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mGraph(red_graph)\n\u001b[1;32m      2\u001b[0m nx\u001b[39m.\u001b[39mdraw(G, with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.Graph(red_graph)\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mGraph(blue_graph)\n\u001b[1;32m      2\u001b[0m nx\u001b[39m.\u001b[39mdraw(G, with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "G = nx.Graph(blue_graph)\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mGraph(no_colour_graph)\n\u001b[1;32m      2\u001b[0m nx\u001b[39m.\u001b[39mdraw(G, with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "G = nx.Graph(no_colour_graph)\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Given the graph shown in the image of theory question 1. Implement from scratch using <b>only numpy</b> both graph convolutions when respecting the coloring and without respecting the coloring (edge relation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2.1\n",
    "Graph Convolution without edge coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35404139, 0.35404139, 0.35404139, 0.35404139, 0.35404139])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the feature matrix\n",
    "# we only have 1 feature per node and all are 1\n",
    "feature_matrix = np.array([[1], [1], [1], [1], [1]])\n",
    "\n",
    "# Initialize the weight matrices using xaiver initialization\n",
    "W1 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W2 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W3 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W4 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W5 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "\n",
    "# Compute the output feature matrix\n",
    "Z1 = np.dot(no_colour_graph, np.dot(feature_matrix, W1))\n",
    "Z2 = np.dot(no_colour_graph, np.dot(feature_matrix, W2))\n",
    "Z3 = np.dot(no_colour_graph, np.dot(feature_matrix, W3))\n",
    "Z4 = np.dot(no_colour_graph, np.dot(feature_matrix, W4))\n",
    "Z = Z1 + Z2 + Z3 + Z4\n",
    "# sigma activation function\n",
    "output = 1 / (1 + np.exp(-Z))\n",
    "output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2.2\n",
    "Graph Convolution with edge coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_blue_graph = np.stack((red_graph, blue_graph))\n",
    "red_blue_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56709075, 0.56709075, 0.56709075, 0.56709075, 0.56709075],\n",
       "       [0.56709075, 0.56709075, 0.56709075, 0.56709075, 0.56709075]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the feature matrix\n",
    "# we only have 1 feature per node and all are 1\n",
    "feature_matrix = np.array([[1], [1], [1], [1], [1]])\n",
    "\n",
    "# Initialize the weight matrices using xaiver initialization\n",
    "W1 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W2 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W3 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W4 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "W5 = np.array([np.random.normal(0, 1/ len(feature_matrix))])\n",
    "\n",
    "# Compute the output feature matrix\n",
    "Z1 = np.dot(red_blue_graph, np.dot(feature_matrix, W1))\n",
    "Z2 = np.dot(red_blue_graph, np.dot(feature_matrix, W2))\n",
    "Z3 = np.dot(red_blue_graph, np.dot(feature_matrix, W3))\n",
    "Z4 = np.dot(red_blue_graph, np.dot(feature_matrix, W4))\n",
    "Z = Z1 + Z2 + Z3 + Z4\n",
    "# sigma activation function\n",
    "output = 1 / (1 + np.exp(-Z))\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now reinterpret the exercise that we had in the last assignment.\n",
    "\n",
    "In the following, we want to train a GNN using the [Deep Graph Library (DGL)](https://github.com/dmlc/dgl) using the data set from last week  ([Tensorflow version](https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews) and [(py)torch version](https://pytorch.org/text/stable/datasets.html#yelpreviewpolarity).\n",
    "\n",
    "Recap:\n",
    "Given the text of a review, we want to determine whether the yelp review is positive and negative. The data set is pre-split into training and test set. Please use the training data to fine-tune your model, while using the test data to evaluate your models performance. This exercise does not necessarily end in having a SOTA model, the goal is for you to use and fine-tune SOTA pretrained large language models.\n",
    "\n",
    "Problem Setting:\n",
    "\n",
    "The label $y$ to a Yelp review $T$ is either positive or negative. Given a Yelp Review $T$ and a polarity feedback $y$ determine whether the Review $T$ is positive or negative. The training set $\\mathcal{D} = \\{(T_1, y_1), \\ldots, (T_N, y_N)\\}$, where $T_i$ is review $i$ and $y_i$ is $T_i$'s polarity feedback. Use <b>AUC</b> as Evaluation Metric.\n",
    "\n",
    "In the following, please solve all subtasks as well as the ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.0\n",
    "Load the Yelp Review Polarity dataset. Here it is also possible to only use part of the data (e.g. 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import YelpReviewPolarity\n",
    "import torch\n",
    "\n",
    "# Load YelpReviewPolarity dataset, split into train and test\n",
    "# datatype: ShardingFilterIterDataPipe\n",
    "train_data, test_data = YelpReviewPolarity(root='.data', split=('train', 'test'))\n",
    "\n",
    "percentage_to_use = 0.000005 #change nr if you want more/less train data\n",
    "\n",
    "train_split_index = int(len(list(train_data)) * percentage_to_use)\n",
    "test_split_index = int(len(list(test_data)) * percentage_to_use)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1\n",
    "Reinterpret the text data as graph data using DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train progess:  0  of  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv_advml/lib/python3.10/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test progess:  0  of  50\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_graphs = []\n",
    "train_split_index = max(train_split_index, 100)\n",
    "train_data = list(train_data)[:train_split_index] \n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    if i % 100 == 0:\n",
    "        print('Train progess: ', i, ' of ', len(train_data))\n",
    "    # split train data instance into label and text inputs\n",
    "    label, inputs = train_data[i]\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized_inputs = tokenizer(inputs, padding=True, add_special_tokens=True,  truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # get embeddings for tokens\n",
    "    mask = tokenized_inputs['attention_mask']\n",
    "    input_id = tokenized_inputs['input_ids']\n",
    "    output = model(input_id, mask)\n",
    "\n",
    "    # initialize graph with nodes\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(output['last_hidden_state'][0]), {'lhs': output['last_hidden_state'][0]})\n",
    "\n",
    "    # get lists of edge sources and destinations (every token has a directed edge to the succeding token)\n",
    "    edge_src = list(range(0, len(output['last_hidden_state'][0])))\n",
    "    edge_dst = list(range(1, len(output['last_hidden_state'][0]) + 1))\n",
    "\n",
    "    # add the edges\n",
    "    g.add_edges(edge_src, edge_dst)\n",
    "\n",
    "    # add self-loop to avoid having 0 in-degree nodes\n",
    "    g = dgl.add_self_loop(g)\n",
    "    \n",
    "    # append graph of this review to list\n",
    "    train_graphs.append((g, label))\n",
    "\n",
    "\n",
    "test_graphs = []\n",
    "test_split_index = max(test_split_index, 50)\n",
    "test_data = list(test_data)[:test_split_index] \n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    if i % 100 == 0:\n",
    "        print('Test progess: ', i, ' of ', len(test_data))\n",
    "    # split test data instance into label and text inputs\n",
    "    label, inputs = test_data[i]\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized_inputs = tokenizer(inputs, padding=True, add_special_tokens=True,  truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # get embeddings for tokens\n",
    "    mask = tokenized_inputs['attention_mask']\n",
    "    input_id = tokenized_inputs['input_ids']\n",
    "    output = model(input_id, mask)\n",
    "\n",
    "    # initialize graph with nodes\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(output['last_hidden_state'][0]), {'lhs': output['last_hidden_state'][0]})\n",
    "\n",
    "    # get lists of edge sources and destinations (every token has a directed edge to the succeding token)\n",
    "    edge_src = list(range(0, len(output['last_hidden_state'][0])))\n",
    "    edge_dst = list(range(1, len(output['last_hidden_state'][0]) + 1))\n",
    "\n",
    "    # add the edges\n",
    "    g.add_edges(edge_src, edge_dst)\n",
    "\n",
    "    # add self-loop to avoid having 0 in-degree nodes\n",
    "    g = dgl.add_self_loop(g)\n",
    "    # append graph of this review to list\n",
    "    test_graphs.append((g, label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2\n",
    "\n",
    "Split the data according to your evaluation protocol. Explain whether this task is inductive or transductive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_graphs_split, val_graphs = train_test_split(train_graphs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract the labels from the data\n",
    "train_labels = torch.tensor([label for _, label in train_graphs_split])\n",
    "val_labels = torch.tensor([label for _, label in val_graphs])\n",
    "\n",
    "# subtract 1 from each element of the labels to get labels in range [0, 1]\n",
    "train_labels = train_labels - 1\n",
    "val_labels = val_labels - 1\n",
    "\n",
    "# Create DGLGraph batches for training, validation, and test sets\n",
    "train_batched_graph = dgl.batch([gr for gr, _ in train_graphs_split])\n",
    "val_batched_graph = dgl.batch([gr for gr, _ in val_graphs])\n",
    "test_batched_graph = dgl.batch([gr for gr, _ in test_graphs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3\n",
    "Implement, evaluate (with the evaluation protocol you defined in Theory Question 2) and train a GCN architecture for the Yelp data set and the problem setting defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Loss: 0.6743, Val Loss: 1.3607, Val AUC: 0.8352\n",
      "Epoch 2/10: Loss: 1.4136, Val Loss: 0.6217, Val AUC: 0.8901\n",
      "Epoch 3/10: Loss: 0.6001, Val Loss: 0.6409, Val AUC: 0.9011\n",
      "Epoch 4/10: Loss: 0.6135, Val Loss: 0.5738, Val AUC: 0.9011\n",
      "Epoch 5/10: Loss: 0.5449, Val Loss: 0.5134, Val AUC: 0.9011\n",
      "Epoch 6/10: Loss: 0.4720, Val Loss: 0.4407, Val AUC: 0.9121\n",
      "Epoch 7/10: Loss: 0.3697, Val Loss: 0.4324, Val AUC: 0.9011\n",
      "Epoch 8/10: Loss: 0.3117, Val Loss: 0.4483, Val AUC: 0.8901\n",
      "Epoch 9/10: Loss: 0.2889, Val Loss: 0.4366, Val AUC: 0.9011\n",
      "Epoch 10/10: Loss: 0.1954, Val Loss: 0.5475, Val AUC: 0.9011\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.conv1(g, features))\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        g.ndata['h'] = x\n",
    "        hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.fc(hg)\n",
    "\n",
    "# Initialize the GCN model\n",
    "in_feats = 768  # Assuming each token is represented by a 768-dimensional vector\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "model = GCN(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(train_batched_graph, train_batched_graph.ndata['lhs'])\n",
    "    loss = criterion(logits, train_labels)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    # evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # get logits and loss on the validation set\n",
    "        logits = model(val_batched_graph, val_batched_graph.ndata['lhs'])\n",
    "        val_loss = criterion(logits, val_labels)\n",
    "        # get the auc score on the validation set\n",
    "        val_prob = F.softmax(logits, dim=1)[:, 1]\n",
    "        val_auc = roc_auc_score(val_labels.numpy(), val_prob.numpy())\n",
    "\n",
    "    # print the epoch, loss, val_loss and validation auc score\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Question 2\n",
    "Given a graph with N vertices and M edges, for three different aggregation functions prove that they are permutation invariant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "In the following,  we will use a dataset as well as pre-defined task. You can find the dataset as well as the task under ogbl-collab [here](https://ogb.stanford.edu/docs/linkprop/#ogbl-collab). Split the data, use their evaluation metric as well as train a GCN for link prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1\n",
    "Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.2\n",
    "Implement, train and evaluate using the data (from Task 4.1.) a GCN using the edge and node information in a geometric DL package of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Please describe your team's implementation of this project, including your personal contribution, in 1000-1500 characters. Each team member must explain the main aspects of the team's implementation, and may not discuss this summary with other students. You are allowed to use figures and tables to clarify. This summary constitutes a separately and individually graded piece of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task:\n",
    "\n",
    "BT 1: Find a useful task where you can combine Transformers and GNNs/Graphs. Explain as clearly as possible why your approach makes sense. This approach should not be given in literature.\n",
    "\n",
    "BT 2: Implement your idea/approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
