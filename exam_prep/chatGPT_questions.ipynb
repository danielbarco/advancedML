{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the calculation of attention weights in the scaled dot-product attention mechanism?\n",
    "\n",
    "A) $\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)$ <br>\n",
    "B) $\\text{Attention}(Q, K, V) = \\text{softmax}(QV^T)$ <br>\n",
    "C) $\\text{Attention}(Q, K, V) = \\text{softmax}(KV^T)$ <br>\n",
    "D) $\\text{Attention}(Q, K, V) = \\text{softmax}(QKV^T)$<br>\n",
    "\n",
    "Explanation: The equation A) $\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)$ represents the calculation of attention weights in the scaled dot-product attention mechanism. In this equation, $Q$, $K$, and $V$ represent the query, key, and value matrices, respectively. The dot product between the query matrix $Q$ and the key matrix $K$ is scaled by a factor of $\\frac{1}{\\sqrt{d_k}}$, where $d_k$ represents the dimensionality of the key vectors. The softmax function is then applied to obtain the attention weights, which determine the importance or relevance of each key-value pair. These attention weights are used to compute the weighted sum of the value matrix $V$ to obtain the output of the attention mechanism.\n",
    "\n",
    "!! the scaling by $\\sqrt{d_k}$ is optional\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) * V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the calculation of multihead attention in the Transformer model?\n",
    "\n",
    "A) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_O$ <br>\n",
    "B) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_K$ <br>\n",
    "C) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_Q$ <br>\n",
    "D) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_V$ <br>\n",
    "\n",
    "In this equation, $Q$, $K$, and $V$ represent the query, key, and value matrices, respectively. The multihead attention mechanism splits these matrices into $h$ different attention heads, each corresponding to a different linear projection matrix ($W_Q$, $W_K$, and $W_V$). Each attention head performs its own scaled dot-product attention calculation using its respective projection matrices. The outputs of all attention heads are then concatenated together, and a final linear projection $W_O$ is applied to obtain the output of the multihead attention mechanism.\n",
    "\n",
    "The linear projection matrix $W_O$ is a learnable parameter of the model. It allows the model to learn the appropriate weights and biases to apply to the concatenated attention head outputs, ensuring that the final output of the multihead attention mechanism aligns with the desired output dimensionality and representation.\n",
    "\n",
    "So, in summary, $W_O$ in the equation represents the linear projection matrix that transforms the concatenated outputs of the individual attention heads in the multihead attention mechanism.\n",
    "\n",
    "Therefore, the correct equation is A) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_O$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the calculation of positional encoding in the Transformer model?\n",
    "\n",
    "A) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "B) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "C) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "D) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "Please select your answer (A, B, C, or D).\n",
    "\n",
    "Explanation: The correct equation for the calculation of positional encoding in the Transformer model is D) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$.\n",
    "\n",
    "In the Transformer model, positional encoding is used to provide information about the relative or absolute positions of the tokens in the input sequence. The positional encoding is added to the input embeddings to incorporate the notion of position into the model's self-attention mechanism.\n",
    "\n",
    "The correct equation represents the calculation of the positional encoding. It applies sine and cosine functions to the positional indices and scales them by different factors of $10000$. The $2i$ and $2i+1$ terms ensure that each dimension of the positional encoding captures different frequencies of the sine and cosine waves. The $d_{\\text{{model}}}$ term represents the dimensionality of the model.\n",
    "\n",
    "!! i+1 on the cos positional encoding is for uneven numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the update gate in the LSTM (Long Short-Term Memory) model?\n",
    "\n",
    "A) $u_t = \\sigma(W_u \\cdot [h_{t-1}, x_t] + b_u)$  <br>\n",
    "B) $u_t = \\tanh(W_u \\cdot [h_{t-1}, x_t] + b_u)$ <br>\n",
    "C) $u_t = \\sigma(W_u \\cdot h_{t-1} + U_u \\cdot x_t + b_u)$ <br>\n",
    "D) $u_t = \\tanh(W_u \\cdot h_{t-1} + U_u \\cdot x_t + b_u)$ <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the equation that represents the value estimation in the N-armed bandit problem?\n",
    "\n",
    "A) $Q_t(a) = R_t(a)$ <br>\n",
    "B) $Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i(a)$ <br>\n",
    "C) $Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{t} R_i(a)$ <br>\n",
    "D) $Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{t} \\mathbb{1}(A_i=a) R_i(a)$ <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The correct equation that represents the value estimation in the N-armed bandit problem is D) $Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{t} \\mathbb{1}(A_i=a) R_i(a)$.\n",
    "\n",
    "In the N-armed bandit problem, an agent faces a set of actions (arms) with unknown reward probabilities. The goal is to maximize the total expected reward over a sequence of time steps. To achieve this, the agent maintains an estimate of the action values, denoted as $Q_t(a)$, at time step $t$ for action $a$. This estimate is updated based on the observed rewards.\n",
    "\n",
    "The correct equation takes into account the number of times action $a$ has been chosen up to time step $t$, denoted as $N_t(a)$. It calculates the average reward obtained from choosing action $a$ by summing the rewards $R_i(a)$ for all time steps $i$ where action $a$ was selected, and then dividing it by the count $N_t(a)$.\n",
    "\n",
    "The indicator function $\\mathbb{1}(A_i=a)$ is used to count the occurrences when action $a$ was chosen at time step $i$. This ensures that only the rewards associated with the chosen action are considered in the update.\n",
    "\n",
    "Therefore, the correct equation is D) $Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{t} \\mathbb{1}(A_i=a) R_i(a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the equation for the upper-confidence-bound (UCB) action selection method in the N-armed bandit problem?\n",
    "\n",
    "A) $A_t = \\text{argmax}_a Q_t(a)$ <br>\n",
    "B) $A_t = \\text{argmax}_a N_t(a)$ <br>\n",
    "C) $A_t = \\text{argmax}_a \\frac{Q_t(a)}{N_t(a)}$ <br>\n",
    "D) $A_t = \\text{argmax}_a \\left(Q_t(a) + c \\sqrt{\\frac{\\log(t)}{N_t(a)}}\\right)$ <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The equation for the upper-confidence-bound (UCB) action selection method in the N-armed bandit problem is indeed given by option D.\n",
    "\n",
    "In this equation, $A_t$ represents the action chosen at time step $t$, $\\text{argmax}_a$ denotes the action that maximizes the expression inside the argument, $Q_t(a)$ is the estimated action value for action $a$ at time step $t$, $N_t(a)$ represents the number of times action $a$ has been selected up to time step $t$, $c$ is a hyperparameter that controls the trade-off between exploration and exploitation, and $\\log(t)$ is the natural logarithm of the total number of time steps $t$.\n",
    "\n",
    "The UCB action selection method balances exploration and exploitation by adding an exploration term to the estimated action value. The exploration term is based on the uncertainty of the estimate, which is quantified by the confidence interval. The term $c \\sqrt{\\frac{\\log(t)}{N_t(a)}}$ represents the exploration bonus, where the exploration increases as the number of times an action has been selected decreases or as the total number of time steps $t$ increases.\n",
    "\n",
    "Therefore, the correct equation for the UCB action selection method is D) $A_t = \\text{argmax}_a \\left(Q_t(a) + c \\sqrt{\\frac{\\log(t)}{N_t(a)}}\\right)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
