{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the calculation of attention weights in the scaled dot-product attention mechanism?\n",
    "\n",
    "A) $\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)$ <br>\n",
    "B) $\\text{Attention}(Q, K, V) = \\text{softmax}(QV^T)$ <br>\n",
    "C) $\\text{Attention}(Q, K, V) = \\text{softmax}(KV^T)$ <br>\n",
    "D) $\\text{Attention}(Q, K, V) = \\text{softmax}(QKV^T)$<br>\n",
    "\n",
    "Explanation: The equation A) $\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)$ represents the calculation of attention weights in the scaled dot-product attention mechanism. In this equation, $Q$, $K$, and $V$ represent the query, key, and value matrices, respectively. The dot product between the query matrix $Q$ and the key matrix $K$ is scaled by a factor of $\\frac{1}{\\sqrt{d_k}}$, where $d_k$ represents the dimensionality of the key vectors. The softmax function is then applied to obtain the attention weights, which determine the importance or relevance of each key-value pair. These attention weights are used to compute the weighted sum of the value matrix $V$ to obtain the output of the attention mechanism.\n",
    "\n",
    "!! the scaling by $\\sqrt{d_k}$ is optional\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) * V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the calculation of multihead attention in the Transformer model?\n",
    "\n",
    "A) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_O$ <br>\n",
    "B) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_K$ <br>\n",
    "C) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_Q$ <br>\n",
    "D) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_V$ <br>\n",
    "\n",
    "In this equation, $Q$, $K$, and $V$ represent the query, key, and value matrices, respectively. The multihead attention mechanism splits these matrices into $h$ different attention heads, each corresponding to a different linear projection matrix ($W_Q$, $W_K$, and $W_V$). Each attention head performs its own scaled dot-product attention calculation using its respective projection matrices. The outputs of all attention heads are then concatenated together, and a final linear projection $W_O$ is applied to obtain the output of the multihead attention mechanism.\n",
    "\n",
    "The linear projection matrix $W_O$ is a learnable parameter of the model. It allows the model to learn the appropriate weights and biases to apply to the concatenated attention head outputs, ensuring that the final output of the multihead attention mechanism aligns with the desired output dimensionality and representation.\n",
    "\n",
    "So, in summary, $W_O$ in the equation represents the linear projection matrix that transforms the concatenated outputs of the individual attention heads in the multihead attention mechanism.\n",
    "\n",
    "Therefore, the correct equation is A) $\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_O$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the calculation of positional encoding in the Transformer model?\n",
    "\n",
    "A) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "B) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "C) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "D) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i+1}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "Please select your answer (A, B, C, or D).\n",
    "\n",
    "Explanation: The correct equation for the calculation of positional encoding in the Transformer model is D) $PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$.\n",
    "\n",
    "In the Transformer model, positional encoding is used to provide information about the relative or absolute positions of the tokens in the input sequence. The positional encoding is added to the input embeddings to incorporate the notion of position into the model's self-attention mechanism.\n",
    "\n",
    "The correct equation represents the calculation of the positional encoding. It applies sine and cosine functions to the positional indices and scales them by different factors of $10000$. The $2i$ and $2i+1$ terms ensure that each dimension of the positional encoding captures different frequencies of the sine and cosine waves. The $d_{\\text{{model}}}$ term represents the dimensionality of the model.\n",
    "\n",
    "!! i+1 on the cos positional encoding is for uneven numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which equation represents the update gate in the LSTM (Long Short-Term Memory) model?\n",
    "\n",
    "A) $u_t = \\sigma(W_u \\cdot [h_{t-1}, x_t] + b_u)$  <br>\n",
    "B) $u_t = \\tanh(W_u \\cdot [h_{t-1}, x_t] + b_u)$ <br>\n",
    "C) $u_t = \\sigma(W_u \\cdot h_{t-1} + U_u \\cdot x_t + b_u)$ <br>\n",
    "D) $u_t = \\tanh(W_u \\cdot h_{t-1} + U_u \\cdot x_t + b_u)$ <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
