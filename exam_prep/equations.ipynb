{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Equations for Advanced Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of the original paper:\n",
    "\n",
    "$d_k$ = 64, dimension of the query and the key <br>\n",
    "$d_v$ = 64, dimension of the value<br>\n",
    "$d_{model}$ = 512 <br>\n",
    "h = 8 <br>\n",
    "$T_x$: variable sequence length; number of keys to compare the query with <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "$Q$ = query  <br>\n",
    "$K^T$ = key <br>\n",
    "$V$ = value <br>\n",
    "$d_k$ = dimension of keys  <br>\n",
    "$T$ = transpose (in order for us to multiply the matrices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_O$\n",
    "\n",
    "$W_O \\in \\mathbb{R}^{h d_v \\times d_{model}}$ \n",
    "\n",
    "$W_O = 512 \\times 512$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PE_i: (pos, i) $ &rarr; $\\sin(\\frac{pos}{10000\\frac{i}{d}})$ if i is even <br>\n",
    "$PE_i: (pos, i) $ &rarr; $\\cos(\\frac{pos}{10000\\frac{i-1}{d}})$ else\n",
    "\n",
    "lecture slides (p.67) pairwise encoding <br>\n",
    "d model dimension 512\n",
    "\n",
    "$\\sin(\\frac{pos}{10000\\frac{0}{d}}), \\cos(\\frac{pos}{10000\\frac{0}{d}}), \\sin(\\frac{pos}{10000\\frac{1}{d}}), \\cos(\\frac{pos}{10000\\frac{1}{d}})$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of the original paper:\n",
    "\n",
    "$d_k$ = 64, dimension of the query and the key <br>\n",
    "$d_v$ = 64, dimension of the value<br>\n",
    "$d_{model}$ = 512 <br>\n",
    "h = 8 <br>\n",
    "$T_x$: variable sequence length; number of keys to compare the query with <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf_c(w,d) = tf(w,d)*idf_c(w)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf$ = term-frequency (count of words)  <br>\n",
    "$idf$ = inverse (log) document frequency (how many time is this word in corpus) <br>\n",
    "$c$ = corpus <br>\n",
    "$w$ = word <br>\n",
    "$d$ = document <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-armed bandit\n",
    "\n",
    "$$Q(a) = Q(a) + \\alpha \\cdot (R - Q(a))$$\n",
    "\n",
    "- Q(a) is the current estimate of the action value for action 'a'.\n",
    "- α (alpha) is the step size or learning rate, determining the weight given to new information. It controls the rate at which the agent updates its value estimates.\n",
    "- R is the reward received by taking action 'a'.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A_t = \\argmax_{a \\in A}(Q_t(a) + c \\sqrt{\\frac{\\ln t }{ N_t(a)}})$$\n",
    "\n",
    "- Q(a) is the estimated value of action \"a\", which represents the average reward obtained from selecting action \"a\" so far. <br>\n",
    "- c is a positive constant that controls the degree of exploration. A higher value of c encourages more exploration.<br>\n",
    "- ln(t) is the natural logarithm of the current time step \"t\".<br>\n",
    "- N(a) is the count of how many times action \"a\" has been selected so far.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation (slides p. 17):\n",
    "\n",
    "state value:\n",
    "$$v_{\\pi}(s) = \\mathbb{E_π}[G_t| S_t = s]$$\n",
    "\n",
    "$G_t$ = return (i.e., cumulative discounted reward) following time t\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_{\\pi}(s')]$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**action-value:**\n",
    "\n",
    "$$q_*(s,a) = \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma \\max_{a'} q_*(s', a')]$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration (slides p. 36)\n",
    "\n",
    "**state value:**\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "first sum is overall everything: \n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)} (\\pi(a|s) \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_{\\pi}(s')])$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sum is the sum over all actions, the second is a sum over all state actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration (book p.83)\n",
    "\n",
    "**state value**\n",
    "$$v_{k + 1}(s) = \\max_{a} \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_k(s')]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "basically take the bellman optimality equation and turn it into an iterative update (which is denoted by k). Get closer and closer to optimal solution by feeding it back to itself iteratively. We have replaced the sum over policy actions with the maximum of action.\n",
    "\n",
    "We can also write it without the k:\n",
    "\n",
    "$$v(s) = \\max_{a} \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v(s')]$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement (slides p.39)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi'(s) = \\argmax_a \\sum_{s', r} p(s', r|s,a)[r+\\gamma v_{\\pi}(s')] $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy vs. Value iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of the value iteration over the policy iteration is that it can converge faster if the state space is large. In problems with more limited state spaces (like the Gridworld), the policy iteration converges with fewer steps, as calculating a good approximation of the action value is easy. Another advantage of the value iteration is that it only relies on the Bellman optimality equation rather than requiring both the Bellman expectation equation and a greedy policy improvement as the policy iteration does. This alteration between the policy evaluation and the policy improvement requires an additional loop, but tends to make the training process of the policy iteration more stable (i.e. less oscillation).\n",
    "Both these algorithms can be applied when we know the MDP to finite environments, where actions, states and rewards are finite too. Common applications are certain boardgames, video games, as well as control and navigation of robots that can be modelled as Markov Decision Processes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA on policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation show how we update action value by incrementally updating the mean $Q(S_t, A_t) $ <br>\n",
    "\n",
    "$S_t, A_t$ start in state, action pair  <br>\n",
    "$R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$ TD error <br>\n",
    "$R_{t+1}$ estimated return after one step we use this one step look ahead to update our mean<br>\n",
    "\n",
    "$R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$ = TD target <br>\n",
    "$\\alpha$ is a constant of how much we update  <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning (SARSA for off policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{q\\phi(z|x)}[\\log \\frac{p(x,z)}{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{\\theta}(\\mathbf{x}|\\mathbf{z})$ is an approximate variational distribution, we want to model teh latent variables given the data samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO Variational Autoencoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{q\\phi(z|x)}[\\log \\frac{p(x,z)}{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] = \\mathbb{E}_{q\\phi(z|x)}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q(x_t|x_{t-1})$  is the forward diffusion process <br>\n",
    "$q_\\phi(z|x)$ tells us how to convert data samples 𝒙 to some latent variables 𝒛 → encoder\n",
    "\n",
    "$p(x_{t-1}|x_t)$ is the reverse denoising process (this is what we want to learn) <br>\n",
    "$p_\\theta(x|z)$ tells us how to convert latent variables 𝒛 to the original data samples 𝒙 → decoder\n",
    "\n",
    "$ \\mathbb{E}_{q\\phi(z|x)}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]$ Reconstruction term (maximise)\n",
    "\n",
    "$D_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$ prior matching term (minimise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://yunfanj.com/blog/2021/01/11/ELBO.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO Hierarchical Variational Autoencoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{q\\phi(z_{1:T}|x)}[\\log \\frac{p(x,z_{1:T})}{q\\phi(z_{1:T}|x)}] = \\mathbb{E}_{q\\phi(z_{1:T}|x)}[\\log \\frac{p(z_T)p_\\theta(x|z_1)\\prod_{t=2}^Tp_\\theta(z_{t-1}|\n",
    "z_t)}{q_\\phi(z_1|x)\\prod_{t=2}^Tq_\\phi(z_t|z_{t-1})}]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO Diffusion models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{ELBO} = \\mathbb{E}_{q(\\mathbf{x_1}|\\mathbf{x_0})}[\\log p_{\\theta}(\\mathbf{x_0}|\\mathbf{x_1})] - D_{KL}(q(\\mathbf{x_T}|\\mathbf{x_0})||p(\\mathbf{x_T})) - \\sum_{t = 2}^T \\mathbb{E}_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}| x_t, x_0)|| p_\\theta(x_{t-1}|x_t))]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0$ = GT data\n",
    "\n",
    "$x_1$ to $x_T$ = latent variables (aka z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_v) = \\sigma(x_v \\sum_{r \\in R}\\sum_{j \\in N_r(v)}c_{jv}\\psi(x_j))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v = vicky <br>\n",
    "$\\sigma$ = add non-linearity <br>\n",
    "$R$ = set of relations  <br>\n",
    "$j$ = user <br>\n",
    "$N_r(v)$ = set of neighbours of 𝑣 with edge relation 𝑟 <br>\n",
    "$\\psi$ = weight <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_advml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
