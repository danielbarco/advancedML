{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Equations for Advanced Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of the original paper:\n",
    "\n",
    "$d_k$ = 64, dimension of the query and the key <br>\n",
    "$d_v$ = 64, dimension of the value<br>\n",
    "$d_{model}$ = 512 <br>\n",
    "h = 8 <br>\n",
    "$T_x$: variable sequence length; number of keys to compare the query with <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "$Q$ = query  <br>\n",
    "$K^T$ = key <br>\n",
    "$V$ = value <br>\n",
    "$d_k$ = dimension of keys  <br>\n",
    "$T$ = transpose (in order for us to multiply the matrices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\text{Multihead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W_O$\n",
    "\n",
    "$W_O \\in \\mathbb{R}^{h d_v \\times d_{model}}$ \n",
    "\n",
    "$W_O = 512 \\times 512$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PE_{(pos,2i)} = \\sin(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$\n",
    "\n",
    "$PE_{(pos,2i+1)} = \\cos(\\frac{{pos}}{{10000^{(\\frac{{2i}}{{d_{\\text{{model}}}}})}}})$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PE_i: (pos, i) $ &rarr; $\\sin(\\frac{pos}{10000\\frac{i}{d}})$ if i is even <br>\n",
    "$PE_i: (pos, i) $ &rarr; $\\cos(\\frac{pos}{10000\\frac{i-1}{d}})$ else\n",
    "\n",
    "lecture slides (p.67) pairwise encoding <br>\n",
    "d model dimension 512\n",
    "\n",
    "$\\sin(\\frac{pos}{10000\\frac{0}{d}}), \\cos(\\frac{pos}{10000\\frac{0}{d}}), \\sin(\\frac{pos}{10000\\frac{1}{d}}), \\cos(\\frac{pos}{10000\\frac{1}{d}})$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of the original paper:\n",
    "\n",
    "$d_k$ = 64, dimension of the query and the key <br>\n",
    "$d_v$ = 64, dimension of the value<br>\n",
    "$d_{model}$ = 512 <br>\n",
    "h = 8 <br>\n",
    "$T_x$: variable sequence length; number of keys to compare the query with <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-armed bandit\n",
    "\n",
    "$$Q(a) = Q(a) + \\alpha \\cdot (R - Q(a))$$\n",
    "\n",
    "- Q(a) is the current estimate of the action value for action 'a'.\n",
    "- Œ± (alpha) is the step size or learning rate, determining the weight given to new information. It controls the rate at which the agent updates its value estimates.\n",
    "- R is the reward received by taking action 'a'.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A_t = \\argmax_{a \\in A}(Q_t(a) + c \\sqrt{\\frac{\\ln t }{ N_t(a)}})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation:\n",
    "\n",
    "state value:\n",
    "$$v_{\\pi}(s) = \\mathbb{E_œÄ}[G_t| S_t = s]$$\n",
    "\n",
    "$G_t$ = return (i.e., cumulative discounted reward) following time t\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_{\\pi}(s')]$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**action-value:**\n",
    "\n",
    "$$q_*(s,a) = \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma \\max_{a'} q_*(s', a')]$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration (slides 36)\n",
    "\n",
    "**state value:**\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "first sum is overall everything: \n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)} (\\pi(a|s) \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_{\\pi}(s')])$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sum is the sum over all actions, the second is a sum over all state actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration (book p.83)\n",
    "\n",
    "**state value**\n",
    "$$v_{k + 1}(s) = \\max_{a} \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v_k(s')]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "basically take the bellman optimality equation and turn it into an iterative update (which is denoted by k). Get closer and closer to optimal solution by feeding it back to itself iteratively. We have replaced the sum over policy actions with the maximum of action.\n",
    "\n",
    "We can also write it without the k:\n",
    "\n",
    "$$v(s) = \\max_{a} \\sum_{s' \\in S, r \\in R} p(s', r|s,a) [r(s,a) + \\gamma v(s')]$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy vs. Value iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of the value iteration over the policy iteration is that it can converge faster if the state space is large. In problems with more limited state spaces (like the Gridworld), the policy iteration converges with fewer steps, as calculating a good approximation of the action value is easy. Another advantage of the value iteration is that it only relies on the Bellman optimality equation rather than requiring both the Bellman expectation equation and a greedy policy improvement as the policy iteration does. This alteration between the policy evaluation and the policy improvement requires an additional loop, but tends to make the training process of the policy iteration more stable (i.e. less oscillation).\n",
    "Both these algorithms can be applied when we know the MDP to finite environments, where actions, states and rewards are finite too. Common applications are certain boardgames, video games, as well as control and navigation of robots that can be modelled as Markov Decision Processes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Variational Autoencoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{q\\phi(z|x)}[\\log \\frac{p(x,z)}{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] = \\mathbb{E}_{q\\phi(z|x)}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q(x_t|x_{t-1})$  is the forward diffusion process <br>\n",
    "$q_\\phi(z|x)$ tells us how to convert data samples ùíô to some latent variables ùíõ ‚Üí encoder\n",
    "\n",
    "$p(x_{t-1}|x_t)$ is the reverse denoising process (this is what we want to learn) <br>\n",
    "$p_\\theta(x|z)$ tells us how to convert latent variables ùíõ to the original data samples ùíô ‚Üí decoder\n",
    "\n",
    "$ \\mathbb{E}_{q\\phi(z|x)}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]$ Reconstruction term (maximise)\n",
    "\n",
    "$D_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$ prior matching term (minimise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://yunfanj.com/blog/2021/01/11/ELBO.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Hierarchical Variational Autoencoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{q\\phi(z_{1:T}|x)}[\\log \\frac{p(x,z_{1:T})}{q\\phi(z_{1:T}|x)}] = \\mathbb{E}_{q\\phi(z_{1:T}|x)}[\\log \\frac{p(z_T)p_\\phi(x|z_1)\\prod_{t=2}^Tp_\\theta(z_{t-1}|\n",
    "z_t)}{q_\\phi(z_1|x)\\prod_{t=2}^Tq_\\phi(z_t|z_{t-1})}]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Diffusion models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{ELBO} = \\mathbb{E}_{q(\\mathbf{x_1}|\\mathbf{x_0})}[\\log p_{\\theta}(\\mathbf{x_0}|\\mathbf{x_1})] - D_{KL}(q(\\mathbf{x_T}|\\mathbf{x_0})||p(\\mathbf{x_T})) - \\sum_{t = 2}^T \\mathbb{E}_{q(x_t|x_0)[D_{KL}(q(x_{t-1}))]}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0$ = GT data\n",
    "\n",
    "$x_1$ to $x_T$ = latent variables (aka z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_advml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
