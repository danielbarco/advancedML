{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How in depth should we know LSTM? All equations in the gate?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much do we need to know tf-idf, Elmo, GRU?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers p. 67, why is i - 1 in denominator whereas in the paper it is in variable?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of things would you like us to know about:\n",
    "\n",
    "– WebGPT <br>\n",
    "– LaMDA <br>\n",
    "– RETRO\n",
    "– Gopher(Cite) <br>\n",
    "– T oolformer <br>\n",
    "– Chain-of-Thought Prompting <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want us to know the scematics of the transformer by heart?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need linear projections in the multihead transformer?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to describe each variable, if we use formulas in the exam?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to no N-armed bandit equation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slides (p.17, 36) what is $\\sum_{a \\in A(s)} π(a|s)$  <br> \n",
    "$π(a|s)$ returns the probability of taking action a when being\n",
    "in state $s$ but what is the sum of?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answered**  <br>\n",
    "What is the difference between <br> \n",
    "$q_π(s,a)$ *value of taking action a in state s under policy*  and <br> \n",
    "$v_π(s)$ *value of state s under policy (expected return)*  ?\n",
    "\n",
    "&rarr; $q_π(s,a)$ shows the immediate benefit of choosing a specific action  <br> \n",
    "&rarr; $v_π(s)$ takes into account the cumulative effect of all future rewards that can be obtained from that state, reflecting the long-term potential of being in that state and following the policy.  <br> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 prehistory of transformers language models 9.3%\n",
    "8 transformers 14.8%\n",
    "9 Neural attention (transformers) 16.7%\n",
    "8 GNN 14.8%\n",
    "12 RL 22.2%\n",
    "12 diffusion 22.2%\n",
    "————\n",
    "54 points\n",
    "\n",
    "No subquestions \n",
    "\n",
    "One multiple choice question. \n",
    "\n",
    "Do not focus on assignments but on the slides…\n",
    "Very common equations will be more important.\n",
    "\n",
    "\n",
    "Explain the Bellman equation mathematically and conceptionally\n",
    "\n",
    "T-idf equation and explanation\n",
    "\n",
    "Difference between ELBO Variational Auto encoders and diffusion models?\n",
    "\n",
    "SARSA on policy vs SARSA and Q learning and SARSA off policy\n",
    "\n",
    "Advantage of transformers.\n",
    "\n",
    "90 min\n",
    "\n",
    "replicability bonus question\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_advml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
