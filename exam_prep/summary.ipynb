{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Methods: \n",
    "Why do we need Monte Carlo? \n",
    "- Learning methods for estimating value functions and discovering optimal policies from experience (i.e., sampling).\n",
    "- No prior knowledge of the environment is assumed/required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need off policy:\n",
    "- MDP model is unknown, but experience can be sampled\n",
    "- MDP model is known, but is too big to use, except by samples\n",
    "\n",
    "On-policy learning\n",
    "- “Learn on the job”\n",
    "- Learn about policy π from experience sampled from π\n",
    "\n",
    "Off-policy learning\n",
    "- “Look over someone’s shoulder”\n",
    "- Learn about policy π from experience sampled from μ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA: <br>\n",
    "1. $S_t, A_t$ start in state, action pair\n",
    "2. $R_{t+1}$ sample from environment to see which reward we end up with\n",
    "3. $S'_{t+1}$ and which state we end up in\n",
    "4. $A'_{t+1}$ sample policy to see which Action we end up with"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
